{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26dd62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. This task will be done in following steps:\n",
    "# first get the webpage https://www.naukri.com/\n",
    "# Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "# Then click the search button.\n",
    "# Then scrape the data for the first 10 jobs results you get.\n",
    "# Finally create a dataframe of the scraped data.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = 'https://www.naukri.com/' \n",
    "driver.get(url)\n",
    "\n",
    "jobsearch = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "jobsearch.send_keys(\"Data Analyst\")\n",
    "\n",
    "location = driver.find_element_by_id('qsb-location-sugg')\n",
    "location.send_keys(\"Bangalore\")\n",
    "search = driver.find_element_by_class_name('btn')\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc2e4cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Job Title  \\\n",
      "1                                 Senior Data Analyst   \n",
      "2    Tcs Hiring For Senior Data Analyst (bfsi domain)   \n",
      "3                               Business Data Analyst   \n",
      "4                               Business Data Analyst   \n",
      "5                                 Senior Data Analyst   \n",
      "6   Tcs Hiring For MDM (master data management) Da...   \n",
      "7                   Senior Data Analyst IDAM Services   \n",
      "8                       Data Analyst/Sr.Data Engineer   \n",
      "9                                 SENIOR DATA ANALYST   \n",
      "10                             Executive Data Analyst   \n",
      "\n",
      "                                    Company Experience  \\\n",
      "1         Gaussian Networks Private Limited    3-5 Yrs   \n",
      "2            Tata Consultancy Services Ltd.   6-11 Yrs   \n",
      "3                          Trigent Software   5-10 Yrs   \n",
      "4                          Trigent Software   5-10 Yrs   \n",
      "5       Virtusa Consulting Services Pvt Ltd   8-12 Yrs   \n",
      "6            Tata Consultancy Services Ltd.   6-11 Yrs   \n",
      "7   GlaxoSmithKline Pharmaceuticals Limited    4-8 Yrs   \n",
      "8        SYREN TECHNOLOGIES PRIVATE LIMITED    4-9 Yrs   \n",
      "9          McAfee Software (India) Pvt. Ltd    6-8 Yrs   \n",
      "10                     Gokaldas Exports Ltd    0-3 Yrs   \n",
      "\n",
      "                                             Location  \n",
      "1               Gurgaon/Gurugram, Bangalore/Bengaluru  \n",
      "2                        Chennai, Bangalore/Bengaluru  \n",
      "3                                 Bangalore/Bengaluru  \n",
      "4                                 Bangalore/Bengaluru  \n",
      "5   Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram...  \n",
      "6                                  (WFH during Covid)  \n",
      "7                        Chennai, Bangalore/Bengaluru  \n",
      "8                                 Bangalore/Bengaluru  \n",
      "9   Hyderabad/Secunderabad, Chennai, Bangalore/Ben...  \n",
      "10                                 (WFH during Covid)  \n"
     ]
    }
   ],
   "source": [
    "job_title = []\n",
    "company = []\n",
    "locations = []\n",
    "xp = []\n",
    "\n",
    "title_tag = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "company_tag = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "location_tag = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "xp_tag = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "\n",
    "for i in title_tag:\n",
    "    job_title.append(i.text)\n",
    "for c in company_tag:\n",
    "    company.append(c.text)\n",
    "for l in location_tag:\n",
    "    locations.append(l.text)\n",
    "for x in xp_tag:\n",
    "    xp.append(x.text)\n",
    "\n",
    "JobDF = pd.DataFrame({\"Job Title\":job_title[0:10],\"Company\":company[0:10],\"Experience\":xp[0:10],\"Location\":locations[0:10]})\n",
    "JobDF.reset_index(drop=True,inplace = True)\n",
    "JobDF.index+= 1\n",
    "\n",
    "print(JobDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ca72c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data. This task will be done in following steps:\n",
    "# first get the webpage https://www.naukri.com/\n",
    "# Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "# Then click the search button.\n",
    "# Then scrape the data for the first 10 jobs results you get.\n",
    "# Finally create a dataframe of the scraped data.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = 'https://www.naukri.com/' \n",
    "driver.get(url)\n",
    "jobsearch = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "\n",
    "jobsearch.send_keys(\"Data Scientist\")\n",
    "location = driver.find_element_by_id('qsb-location-sugg')\n",
    "location.send_keys(\"Bangalore\")\n",
    "search = driver.find_element_by_class_name('btn')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d94efcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Job Title                           Company  \\\n",
      "1            Hiring For Data Scientist    Tata Consultancy Services Ltd.   \n",
      "2             Lead Data Scientist BFSI            IBM India Pvt. Limited   \n",
      "3             Associate Data Scientist             Philips India Limited   \n",
      "4                       Data Scientist                           Brillio   \n",
      "5              Data Scientist - I / II                         Sharechat   \n",
      "6                       Data Scientist  Allegis Services India Pvt. Ltd.   \n",
      "7                       Data Scientist  Allegis Services India Pvt. Ltd.   \n",
      "8   Data Scientist: Advanced Analytics            IBM India Pvt. Limited   \n",
      "9                Senior Data Scientist                      Walmart Labs   \n",
      "10               Senior Data Scientist                            Airbnb   \n",
      "\n",
      "                                            Location  \\\n",
      "1   Chennai, Bangalore/Bengaluru, Mumbai (All Areas)   \n",
      "2                                Bengaluru/Bangalore   \n",
      "3                                Bangalore/Bengaluru   \n",
      "4                                Bangalore/Bengaluru   \n",
      "5                                Bangalore/Bengaluru   \n",
      "6                                Bangalore/Bengaluru   \n",
      "7                                Bangalore/Bengaluru   \n",
      "8                                Bengaluru/Bangalore   \n",
      "9                                Bangalore/Bengaluru   \n",
      "10                               Bangalore/Bengaluru   \n",
      "\n",
      "                                      Job Description  \n",
      "1   Minumum 3 years of experience in Data Science/...  \n",
      "2   Use predictive modeling to increase and optimi...  \n",
      "3   A day in the Life of Data Scientist at Brillio...  \n",
      "4   Responsibilities Apply state of the art in the...  \n",
      "5   Must Have skill sets:  Excellent knowledge of ...  \n",
      "6   As a Senior Data Scientist for Walmart, you ll...  \n",
      "7   Responsibilities include: Defining and evaluat...  \n",
      "8     Skills Required Skills: Data Science, Machin...  \n",
      "9   Introduction Software Developers at IBM are th...  \n",
      "10  Must Have skill sets:  Excellent knowledge of ...  \n"
     ]
    }
   ],
   "source": [
    "job_title = []\n",
    "company = []\n",
    "locations = []\n",
    "desc_url = []\n",
    "title_tag = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "company_tag = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "location_tag = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span[1]\")\n",
    "description_url = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "for i in title_tag:\n",
    "    job_title.append(i.text)\n",
    "for c in company_tag:\n",
    "    company.append(c.text)\n",
    "for l in location_tag:\n",
    "    locations.append(l.text)\n",
    "for u in description_url:\n",
    "    desc_url.append(u.get_attribute('href'))\n",
    "\n",
    "desc = []\n",
    "for d in desc_url:\n",
    "    driver.get(d)\n",
    "    try:\n",
    "        content= driver.find_element_by_xpath(\"//div[@class='dang-inner-html']\").text.replace('\\n',\" \").replace('\\n\\n',\" \")\n",
    "        \n",
    "        desc.append(content)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "desc2 = []\n",
    "for d in desc_url:\n",
    "    driver.get(d)\n",
    "    try:\n",
    "        content= driver.find_element_by_xpath(\"//div[@class='clearboth description']\").text.replace('\\n',\" \").replace('\\n\\n',\" \")\n",
    "        \n",
    "        desc2.append(content)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "descriptions = [] #combining values of desc1 and desc2 into main 'descriptions' list\n",
    "for i in range(0,8):\n",
    "    descriptions.append(desc[i])\n",
    "for j in range(0,2):\n",
    "    descriptions.append(desc2[j])\n",
    "\n",
    "JobDF = pd.DataFrame({\"Job Title\":job_title[0:10],\"Company\":company[0:10],\"Location\":locations[0:10],\"Job Description\":descriptions[0:10]})\n",
    "JobDF.reset_index(drop=True,inplace = True)\n",
    "JobDF.index+= 1\n",
    "\n",
    "print(JobDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbbdaf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below: You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "# The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs The task will be done as shown in the below steps:\n",
    "# 1.first get the webpage https://www.naukri.com/\n",
    "# 2.Enter “Data Scientist” in “Skill,Designations,Companies” field .\n",
    "# 3.Then click the search button.\n",
    "# 4.Then apply the location filter and salary filter by checking the respective boxes\n",
    "# 5.Then scrape the data for the first 10 jobs results you get.\n",
    "# 6.Finally create a dataframe of the scraped data.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = 'https://www.naukri.com/' \n",
    "driver.get(url)\n",
    "jobsearch = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "jobsearch.send_keys(\"Data Scientist\")\n",
    "\n",
    "search = driver.find_element_by_class_name('btn')\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4044e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_filter = driver.find_element_by_xpath('/html/body/div/div[3]/div[2]/section/div[2]/div/div[2]/div[3]')\n",
    "location_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a57d1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_filter = driver.find_element_by_xpath('/html/body/div/div[3]/div[2]/section/div[2]/div[4]/div[2]/div')\n",
    "salary_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f587c181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "1     Opportunity with Accionlabs For Data Scientist   \n",
      "2                           Manager - Data Scientist   \n",
      "3                                     Data Scientist   \n",
      "4   Data Scientist For IT Company Bangalore location   \n",
      "5                              Data Scientist Intern   \n",
      "6     Senior Associate, Data Hierarchy Analyst( MDM)   \n",
      "7                  Hiring For Data Analyst (DA Role)   \n",
      "8     Tcs Hiring For Azure Data Engineer - PAN India   \n",
      "9                             Executive Data Analyst   \n",
      "10                   Associate Analyst- Data Privacy   \n",
      "\n",
      "                              Company Experience  \\\n",
      "1   Accion labs India Private Limited    3-5 Yrs   \n",
      "2       GENPACT India Private Limited   7-10 Yrs   \n",
      "3                ILABZ TECHNOLOGY LLP    3-5 Yrs   \n",
      "4           SHRI GURU DAYA CONSULTING    0-2 Yrs   \n",
      "5                              TalPro    0-1 Yrs   \n",
      "6       GENPACT India Private Limited    1-3 Yrs   \n",
      "7                           COLLABERA    0-1 Yrs   \n",
      "8      Tata Consultancy Services Ltd.    3-8 Yrs   \n",
      "9                Gokaldas Exports Ltd    0-3 Yrs   \n",
      "10                              EYGBS    1-3 Yrs   \n",
      "\n",
      "                                             Location  \n",
      "1   Hyderabad/Secunderabad, Pune, Bangalore/Bengal...  \n",
      "2                                 Bangalore/Bengaluru  \n",
      "3   Hyderabad/Secunderabad, Pune, Chennai, Bangalo...  \n",
      "4                                 Bangalore/Bengaluru  \n",
      "5                                 Bangalore/Bengaluru  \n",
      "6                                 Bangalore/Bengaluru  \n",
      "7                                 Bangalore/Bengaluru  \n",
      "8                  Pune, Chennai, Bangalore/Bengaluru  \n",
      "9                                 Bangalore/Bengaluru  \n",
      "10  Noida, Hyderabad/Secunderabad, Pune, Delhi / N...  \n"
     ]
    }
   ],
   "source": [
    "job_title = []\n",
    "company = []\n",
    "locations = []\n",
    "xp = []\n",
    "\n",
    "title_tag = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "company_tag = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "location_tag = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span[1]\")\n",
    "xp_tag = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "\n",
    "for i in title_tag:\n",
    "    job_title.append(i.text)\n",
    "for c in company_tag:\n",
    "    company.append(c.text)\n",
    "company\n",
    "for l in location_tag:\n",
    "    locations.append(l.text)\n",
    "for x in xp_tag:\n",
    "    xp.append(x.text)\n",
    "    \n",
    "JobDF = pd.DataFrame({\"Job Title\":job_title[0:10],\"Company\":company[0:10],\"Experience\":xp[0:10],\"Location\":locations[0:10]})\n",
    "JobDF.reset_index(drop=True,inplace = True)\n",
    "JobDF.index+= 1\n",
    "\n",
    "print(JobDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "047f8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, job_title. This task will be done in following steps:\n",
    "# first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "# Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field.\n",
    "# Then click the search button. You will land up in the below page:\n",
    "# Then scrape the data for the first 10 jobs results you get in the above shown page. .\n",
    "# Finally create a dataframe of the scraped data.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "url='https://www.glassdoor.co.in/developer/index.htm'\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "82b7bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = driver.find_element_by_id(\"sc.keyword\")\n",
    "job_title.send_keys(\"Data Scientist\")\n",
    "\n",
    "location = driver.find_element_by_id('sc.location')\n",
    "location.send_keys(\"Noida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "573e64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element_by_xpath('/html/body/header/nav/div/div/div/div[3]/div[2]/form/div/button/span')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "45fbed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Applied Materials Inc.', 'Indeed', 'Quor Technologies, Inc.', 'Data Tale', 'Top Mentor', 'IDFC First Bank Ltd', 'Argoid', 'HP', 'Patient Bond India Private Limited', 'Microsoft', 'Ishatva Management Solutions', 'Argoid', 'Lenskart', 'HDFC Bank', 'Walmart Global Tech India', 'UST Global', 'Crypto Mize', 'Exa Mobility India Private Limited', 'Aera Technology', 'sharechat', 'Deloitte', 'Merkle Sokrati', 'The Learning Zone', 'Reach52, Inc.', 'XCEEDANCE', 'Airtel India', 'Merkle', 'ING', 'Michelin', 'Saishaa Services']\n",
      "['30d+', '30d+', '6d', '19d', '5d', '4d', '7d', '12d', '24h', '24h', '15d', '24h', '7d', '18d', '5d', '30d+', '27d', '5d', '18d', '4d', '8d', '3d', '5d', '10d', '8d', '25d', '3d', '10d', '28d', '21d']\n",
      "['Data Scientist', 'Product: Data Scientist', 'Machine Learning Engineer / Data Scientist', 'Data Scientist', '', 'Associate Data Scientist', 'Data Scientist Intern', 'Data Scientist', 'Data Scientist', 'Data & Applied Scientist', 'Data Scientist', 'Data Scientist (min 1year DS exp) Immed Hire', 'Data Scientist', 'Data Scientist-CAI', 'Data Scientist', 'Data Scientist I', 'Data Scientist', 'Data Science part time job/internship at Pune, Pimpri-Chinchwad', 'Data Scientist', 'Data Scientist - I/II', 'Data Scientist', 'Data Analytics - Intern', 'Trainer for Data Science & AIML', 'Data Scientist', 'DATA SCIENTIST – MACHINE LEARNING', 'Data Scientist', 'Data Analytics - Intern', 'Data Scientist', 'Data Scientist 2', 'Data Scientist']\n"
     ]
    }
   ],
   "source": [
    "company = []\n",
    "\n",
    "company_tags = driver.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between align-items-start']/a\")\n",
    "for i in company_tags:\n",
    "    company.append(i.text)\n",
    "    \n",
    "# print (company)\n",
    "\n",
    "no_days = []\n",
    "\n",
    "no_days_tags = driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-17n8uzw']\")\n",
    "for i in no_days_tags:\n",
    "    no_days.append(i.text)\n",
    "# print (no_days)\n",
    "\n",
    "job_title=driver.find_elements_by_xpath(\"//a[@class='jobLink job-search-key-1rd3saf eigr9kq1']\")\n",
    "title=[]\n",
    "for i in job_title:\n",
    "    title.append(i.text)\n",
    "# print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4668d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Job_Title  \\\n",
      "1                               Data Scientist   \n",
      "2                      Product: Data Scientist   \n",
      "3   Machine Learning Engineer / Data Scientist   \n",
      "4                               Data Scientist   \n",
      "5                                                \n",
      "6                     Associate Data Scientist   \n",
      "7                        Data Scientist Intern   \n",
      "8                               Data Scientist   \n",
      "9                               Data Scientist   \n",
      "10                    Data & Applied Scientist   \n",
      "\n",
      "                               Company Job Age  \n",
      "1               Applied Materials Inc.    30d+  \n",
      "2                               Indeed    30d+  \n",
      "3              Quor Technologies, Inc.      6d  \n",
      "4                            Data Tale     19d  \n",
      "5                           Top Mentor      5d  \n",
      "6                  IDFC First Bank Ltd      4d  \n",
      "7                               Argoid      7d  \n",
      "8                                   HP     12d  \n",
      "9   Patient Bond India Private Limited     24h  \n",
      "10                           Microsoft     24h  \n"
     ]
    }
   ],
   "source": [
    "JobDF = pd.DataFrame({\"Job_Title\":title[0:10],\"Company\":company[0:10],\"Job Age\":no_days[0:10]})\n",
    "JobDF.reset_index(drop=True,inplace = True)\n",
    "JobDF.index+= 1\n",
    "print(JobDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e73a3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location. \n",
    "# You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary and rating.\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = 'https://www.glassdoor.co.in/Salaries/index.htm' \n",
    "driver.get(url)\n",
    "jobsearch = driver.find_element_by_id('KeywordSearch')\n",
    "jobsearch\n",
    "jobsearch.send_keys(\"Data Scientist\")\n",
    "location = driver.find_element_by_id('LocationSearch')\n",
    "location.send_keys(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37de4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element_by_xpath('/html/body/div[3]/div/div[1]/div[1]/div/div/form/button')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2abbb889",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "job_title = []\n",
    "Rating = []\n",
    "sals = []\n",
    "avg_sal = []\n",
    "sal_num = []\n",
    "Rating_tag = driver.find_elements_by_xpath(\"//span[@class='m-0 css-kyx745']\")\n",
    "company_tag = driver.find_elements_by_xpath(\"//a[@class='css-f3vw95 e1aj7ssy3']\")\n",
    "title_tag = driver.find_elements_by_xpath(\"//div[@class='col-12 col-lg px-xsm']//span\")\n",
    "sal_num_tag = driver.find_elements_by_xpath(\"//div[@class='col-12 col-lg-auto']//span\")\n",
    "sals_tag = driver.find_elements_by_xpath(\"//div[@class='d-none d-lg-block']//p\")\n",
    "avg_sal_tag = driver.find_elements_by_xpath(\"//div[@class='col-12 col-lg-4 px-lg-0 d-flex align-items-baseline']\")\n",
    "for i in company_tag:\n",
    "    company.append(i.text)\n",
    "for j in title_tag :\n",
    "    job_title.append(j.text)\n",
    "for r in Rating_tag:\n",
    "    Rating.append(r.text)\n",
    "for n in sal_num_tag:\n",
    "    sal_num.append(n.text)\n",
    "for m in sals_tag:\n",
    "    sals.append(m.text)\n",
    "for a in avg_sal_tag:\n",
    "    avg_sal.append(a.text.replace(\"\\n \",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b17a0e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Job Title                              Company  \\\n",
      "1             Data Scientist            Tata Consultancy Services   \n",
      "2             Data Scientist                           Innoplexus   \n",
      "3             Data Scientist                                  IBM   \n",
      "4             Data Scientist                       Wolters Kluwer   \n",
      "5   Data Scientist - Monthly  AlgoAnalytics Financial Consultancy   \n",
      "6             Data Scientist       Cognizant Technology Solutions   \n",
      "7             Data Scientist                        ZS Associates   \n",
      "8             Data Scientist                            Accenture   \n",
      "9             Data Scientist                  Zensar Technologies   \n",
      "10            Data Scientist             ThinkBumblebee Analytics   \n",
      "\n",
      "   Minimum Salary(₹) Maximum Salary(₹) Average Salary(₹) Number of Salaries  \\\n",
      "1                ₹3L              ₹25L      ₹7,15,000/yr        28 salaries   \n",
      "2                ₹2L              ₹62L     ₹11,68,173/yr        17 salaries   \n",
      "3                ₹3L              ₹51L     ₹13,00,087/yr         9 salaries   \n",
      "4               ₹10L              ₹98L     ₹11,53,327/yr         9 salaries   \n",
      "5               ₹41T              ₹88T        ₹61,657/mo         9 salaries   \n",
      "6                ₹5L              ₹19L      ₹7,65,693/yr         8 salaries   \n",
      "7                ₹1L              ₹23L     ₹12,99,587/yr         8 salaries   \n",
      "8                ₹4L              ₹31L      ₹6,02,413/yr         7 salaries   \n",
      "9                ₹5L              ₹16L      ₹8,58,623/yr         7 salaries   \n",
      "10               ₹5L              ₹13L     ₹10,75,126/yr         7 salaries   \n",
      "\n",
      "   Rating  \n",
      "1     3.9  \n",
      "2     3.7  \n",
      "3     3.9  \n",
      "4     3.8  \n",
      "5     4.3  \n",
      "6     3.8  \n",
      "7       4  \n",
      "8     4.1  \n",
      "9     3.6  \n",
      "10    3.4  \n"
     ]
    }
   ],
   "source": [
    "min_sal =[]\n",
    "max_sal = []\n",
    "\n",
    "for i in range(0,len(sals),2):\n",
    "    min_sal.append(sals[i])\n",
    "for j in range(1,len(sals),2):\n",
    "    max_sal.append(sals[j])\n",
    "\n",
    "JobDF = pd.DataFrame({\"Job Title\":job_title[0:10],\"Company\":company[0:10],\"Minimum Salary(₹)\":min_sal[0:10],\"Maximum Salary(₹)\":max_sal[0:10],\"Average Salary(₹)\":avg_sal[0:10],\"Number of Salaries\":sal_num[0:10],\"Rating\":Rating[0:10]})\n",
    "JobDF.reset_index(drop=True,inplace = True)\n",
    "JobDF.index+= 1\n",
    "print(JobDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c37321f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "# Brand\n",
    "# Product Description\n",
    "# Price\n",
    "# Discount %\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = 'https://www.flipkart.com' \n",
    "driver.get(url)\n",
    "glassessearch = driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "glassessearch.send_keys(\"Sunglasses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1e8bd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "36fc463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element_by_class_name('L0Z3Pu')\n",
    "search.click()\n",
    "\n",
    "brand = []\n",
    "desc = []\n",
    "price = []\n",
    "disc = []\n",
    "\n",
    "brand_tag = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "price_tag = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "disc_tag = driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "desc_tag = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "\n",
    "for b in brand_tag:\n",
    "    brand.append(b.text)\n",
    "for d in desc_tag :\n",
    "    desc.append(d.text)\n",
    "for p in price_tag:\n",
    "    price.append(p.text)\n",
    "for c in disc_tag:\n",
    "    disc.append(c.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "886f491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "nextbtn = driver.find_element_by_class_name('_1LKTO3')\n",
    "while(i<2):\n",
    "    try:\n",
    "        nextbtn.click()\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(3) #to allow the page to load completely before scraping.\n",
    "    brand_tag = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    price_tag = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    disc_tag = driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "    desc_tag = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    \n",
    "    for b in brand_tag:\n",
    "        try:\n",
    "            brand.append(b.text)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for d in desc_tag :\n",
    "        try:\n",
    "            desc.append(d.text)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for p in price_tag:\n",
    "        try:\n",
    "            price.append(p.text)\n",
    "        except:\n",
    "            pass\n",
    "    for c in disc_tag:\n",
    "        try:\n",
    "            disc.append(c.text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    if 'inactive' in nextbtn.get_attribute('class'):\n",
    "        break;\n",
    "    \n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2a1bd8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Brand Name Price(₹) Discount  \\\n",
      "1       New Specs     ₹299  81% off   \n",
      "2    Silver Kartz     ₹284  76% off   \n",
      "3      PHENOMENAL     ₹399  80% off   \n",
      "4        Fastrack   ₹1,085  16% off   \n",
      "5          GANSTA     ₹295  85% off   \n",
      "..            ...      ...      ...   \n",
      "96     PHENOMENAL     ₹319  77% off   \n",
      "97         GANSTA     ₹221  78% off   \n",
      "98         GANSTA     ₹230  70% off   \n",
      "99         AISLIN     ₹498  68% off   \n",
      "100     ROYAL SON     ₹474  78% off   \n",
      "\n",
      "                                           Description  \n",
      "1    Mirrored, UV Protection, Riding Glasses, Other...  \n",
      "2                   UV Protection Oval Sunglasses (56)  \n",
      "3    UV Protection, Mirrored Retro Square Sunglasse...  \n",
      "4                UV Protection Aviator Sunglasses (58)  \n",
      "5    UV Protection, Night Vision, Riding Glasses Av...  \n",
      "..                                                 ...  \n",
      "96     UV Protection Clubmaster Sunglasses (Free Size)  \n",
      "97    UV Protection, Mirrored Wayfarer Sunglasses (53)  \n",
      "98               UV Protection Aviator Sunglasses (57)  \n",
      "99     UV Protection, Gradient Cat-eye Sunglasses (61)  \n",
      "100         UV Protection Retro Square Sunglasses (58)  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "SunglassesDF = pd.DataFrame({\"Brand Name\":brand[0:100],\"Price(₹)\":price[0:100],\"Discount\":disc[0:100],\"Description\":desc[0:100],})\n",
    "SunglassesDF.reset_index(drop=True,inplace = True)\n",
    "SunglassesDF.index+= 1\n",
    "SunglassesDF.shape\n",
    "\n",
    "print(SunglassesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d37ac198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone.Scrape the data for first 100 reviews.\n",
    "\n",
    "# Rating\n",
    "# Review_summary\n",
    "# Full review\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace' \n",
    "driver.get(url)\n",
    "\n",
    "viewall = driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div/div/div[5]/div/a/div/span')\n",
    "viewall.click()\n",
    "\n",
    "rev_sum = []\n",
    "rating = []\n",
    "rev = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "30b3d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_sum_tag = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "rating_tag = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "rev_tag = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "    \n",
    "\n",
    "for r in rev_sum_tag:\n",
    "    rev_sum.append(r.text)\n",
    "for j in rating_tag :\n",
    "    rating.append(j.text)\n",
    "for k in rev_tag:\n",
    "    rev.append(k.text.replace(\"\\n\",\" \"))\n",
    "    \n",
    "i=0\n",
    "nextbtn = driver.find_element_by_class_name('_1LKTO3')\n",
    "while(i<10):\n",
    "    try:\n",
    "        nextbtn.click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    time.sleep(3) #to allow the page to load completely before scraping.\n",
    "    rev_sum_tag = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    rating_tag = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    rev_tag = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "    for r in rev_sum_tag:\n",
    "        try:\n",
    "            rev_sum.append(r.text)\n",
    "        except:\n",
    "            pass\n",
    "    for j in rating_tag :\n",
    "        try:\n",
    "            rating.append(j.text)\n",
    "        except:\n",
    "            pass\n",
    "    for k in rev_tag:\n",
    "        try:\n",
    "            rev.append(k.text.replace(\"\\n\",\" \"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "79ffbb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Summary                                        Full Review  \\\n",
      "1              Brilliant  The Best Phone for the Money  The iPhone 11 of...   \n",
      "2         Simply awesome  Really satisfied with the Product I received.....   \n",
      "3    Best in the market!  Great iPhone very snappy experience as apple k...   \n",
      "4       Perfect product!  Amazing phone with great cameras and better ba...   \n",
      "5              Fabulous!  This is my first iOS phone. I am very happy wi...   \n",
      "..                   ...                                                ...   \n",
      "96     Worth every penny  Best budget Iphone till date ❤️ go for it guys...   \n",
      "97      Perfect product!  Iphone is just awesome.. battery backup is ver...   \n",
      "98        Simply awesome  Excellent camera, good performance, no lag. Th...   \n",
      "99     Worth every penny  It’s been almost a month since I have been usi...   \n",
      "100             Terrific  Really worth of money. i just love it. It is t...   \n",
      "\n",
      "    Rating  \n",
      "1        5  \n",
      "2        5  \n",
      "3        5  \n",
      "4        5  \n",
      "5        5  \n",
      "..     ...  \n",
      "96       5  \n",
      "97       5  \n",
      "98       5  \n",
      "99       5  \n",
      "100      5  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "iphoneDF = pd.DataFrame({\"Summary\":rev_sum[0:100],\"Full Review\":rev[0:100],\"Rating\":rating[0:100]})\n",
    "iphoneDF.reset_index(drop=True,inplace = True)\n",
    "iphoneDF.index+= 1\n",
    "iphoneDF.shape\n",
    "\n",
    "print(iphoneDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "38052e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field. You have to scrape 4 attributes of each sneaker :¶\n",
    "\n",
    "# Brand\n",
    "# Product Description\n",
    "# Price\n",
    "# discount %\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = 'https://www.flipkart.com' \n",
    "driver.get(url)\n",
    "Sneakerssearch = driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "Sneakerssearch.send_keys(\"Sneakers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "13c81b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element_by_class_name('L0Z3Pu')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f1f393cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = []\n",
    "desc = []\n",
    "price = []\n",
    "disc = []\n",
    "brand_tag = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "price_tag = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "disc_tag = driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "desc_tag = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "for b in brand_tag:\n",
    "   brand.append(b.text)\n",
    "for d in desc_tag :\n",
    "   desc.append(d.text)\n",
    "for p in price_tag:\n",
    "   price.append(p.text)\n",
    "for c in disc_tag:\n",
    "   disc.append(c.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "db931533",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "nextbtn = driver.find_element_by_class_name('_1LKTO3')\n",
    "while(i<2):\n",
    "    try:\n",
    "        nextbtn.click()\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(3) #to allow the page to load completely before scraping.\n",
    "    brand_tag = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    price_tag = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    disc_tag = driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "    desc_tag = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    \n",
    "    for b in brand_tag:\n",
    "        try:\n",
    "            brand.append(b.text)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for d in desc_tag :\n",
    "        try:\n",
    "            desc.append(d.text)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for p in price_tag:\n",
    "        try:\n",
    "            price.append(p.text)\n",
    "        except:\n",
    "            pass\n",
    "    for c in disc_tag:\n",
    "        try:\n",
    "            disc.append(c.text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    if 'inactive' in nextbtn.get_attribute('class'):\n",
    "        break;\n",
    "    \n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "55f8e32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Brand Name Price(₹) Discount  \\\n",
      "1        Echor     ₹449  55% off   \n",
      "2        Echor     ₹429  57% off   \n",
      "3     Magnolia     ₹356  64% off   \n",
      "4       BRUTON     ₹474  86% off   \n",
      "5     ASTEROID     ₹499  75% off   \n",
      "..         ...      ...      ...   \n",
      "96      BRUTON     ₹420  78% off   \n",
      "97       BIRDE     ₹283  43% off   \n",
      "98        PUMA   ₹1,847  44% off   \n",
      "99        PUMA   ₹2,006  42% off   \n",
      "100      3SIX5     ₹526  82% off   \n",
      "\n",
      "                                           Description  \n",
      "1                                     Sneakers For Men  \n",
      "2                              Raptor Sneakers For Men  \n",
      "3                                     Sneakers For Men  \n",
      "4    Combo Pack Of 4 Casual Shoes Loafer Shoes Snea...  \n",
      "5    Original Luxury Branded Fashionable Men's Casu...  \n",
      "..                                                 ...  \n",
      "96                            CLARKIN Sneakers For Men  \n",
      "97          Birde Trendy Casual Shoes Sneakers For Men  \n",
      "98                                    Sneakers For Men  \n",
      "99                                    Sneakers For Men  \n",
      "100                      3 Pair Combo Sneakers For Men  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "SneakersDF = pd.DataFrame({\"Brand Name\":brand[0:100],\"Price(₹)\":price[0:100],\"Discount\":disc[0:100],\"Description\":desc[0:100],})\n",
    "SneakersDF.reset_index(drop=True,inplace = True)\n",
    "SneakersDF.index+= 1\n",
    "SneakersDF.shape\n",
    "\n",
    "print(SneakersDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c0b2dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9: Go to the link - https://www.myntra.com/shoes Set Price filter to “Rs. 6649 to Rs. 13099” , \n",
    "# Color filter to “Black”,And then scrape First 100 shoes data you get. \n",
    "#         The data should include “Brand” of the shoes , Short Shoe description, price of the shoe.\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = 'https://www.myntra.com/shoes' \n",
    "driver.get(url)\n",
    "\n",
    "colour_filter = driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label\")\n",
    "colour_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3af3d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_filter = driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "price_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1aa1fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = []\n",
    "desc = []\n",
    "price = []\n",
    "price2 = []\n",
    "brand_tag = driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "price_tag = driver.find_elements_by_xpath(\"//div[@class='product-price']//span[1]\")\n",
    "desc_tag = driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "\n",
    "for b in brand_tag:\n",
    "   brand.append(b.text)\n",
    "for d in desc_tag :\n",
    "   desc.append(d.text)\n",
    "for p in price_tag:\n",
    "   price.append(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b4d56ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "nextbtn = driver.find_element_by_class_name('pagination-next')\n",
    "while(i<1):\n",
    "    try:\n",
    "        nextbtn.click()\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(3) #to allow the page to load completely before scraping.\n",
    "    brand_tag = driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "    price_tag = driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']//span[1]\")\n",
    "    desc_tag = driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "\n",
    "    \n",
    "    for b in brand_tag:\n",
    "        try:\n",
    "            brand.append(b.text)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for d in desc_tag :\n",
    "        try:\n",
    "            desc.append(d.text)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for p in price_tag:\n",
    "        try:\n",
    "            price.append(p.text)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    if 'inactive' in nextbtn.get_attribute('class'):\n",
    "        break;\n",
    "    \n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f6792cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Brand Name   Price(₹)                     Description\n",
      "1                    Puma   Rs. 6999  Men Cell Fraction Fade Running\n",
      "2                    Puma   Rs. 6999  Men BlackTraining or Gym Shoes\n",
      "3               J.FONTINI   Rs. 8490       Men Black Leather Loafers\n",
      "4                    Puma   Rs. 6999       Mesh Hybrid Fuego Running\n",
      "5                    Puma  Rs. 10499     Women Deviate Nitro Running\n",
      "..                    ...        ...                             ...\n",
      "96           Kenneth Cole  Rs. 10493             Women Leather Pumps\n",
      "97   Heel & Buckle London   Rs. 7192            Women Peep Toe Heels\n",
      "98                   Geox  Rs. 10999              Men Solid Sneakers\n",
      "99                   Geox  Rs. 11999          Men Leather Flat Boots\n",
      "100                  Geox   Rs. 9999       Men Leather Driving Shoes\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "while(\"\" in price) :\n",
    "    price.remove(\"\")\n",
    "    \n",
    "j = 0\n",
    "for c in price:\n",
    "    if len(c) > 12:\n",
    "        del price[j]\n",
    "    j += 1\n",
    "    \n",
    "MyntraShoesDF = pd.DataFrame({\"Brand Name\":brand[0:100],\"Price(₹)\":price[0:100],\"Description\":desc[0:100],})\n",
    "MyntraShoesDF.reset_index(drop=True,inplace = True)\n",
    "MyntraShoesDF.index+= 1\n",
    "\n",
    "\n",
    "print(MyntraShoesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "32cb8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the You have to scrape 3 attributes for each laptop:\n",
    "# title\n",
    "# Ratings\n",
    "# Price\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = 'https://www.amazon.in/'\n",
    "driver.get(url)\n",
    "\n",
    "Laptopsearch = driver.find_element_by_id('twotabsearchtextbox')\n",
    "Laptopsearch.send_keys(\"Laptop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b3e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search1 = driver.find_elements_by_xpath('/html/body/div/header/div/div/div[2]/div/form/div[3]/div/span/input')\n",
    "search1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "44c93d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i7_filter = driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in i7_filter:\n",
    "    if i.text=='Intel Core i7':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e71b6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i9_filter = driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in i9_filter:\n",
    "    if i.text=='Intel Core i9':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "47c09c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "price = []\n",
    "Rating = []\n",
    "Rating_tag = driver.find_elements_by_xpath(\"//span[@class='a-icon-alt']\")\n",
    "title_tag = driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "price_tag = driver.find_elements_by_xpath(\"//div[@class='sg-col-inner']//span[@class='a-price-whole']\")\n",
    "\n",
    "for i in title_tag:\n",
    "    title.append(i.text)\n",
    "    \n",
    "for r in Rating_tag :\n",
    "    Rating.append(r.get_attribute(\"innerHTML\"))\n",
    "    \n",
    "for p in price_tag:\n",
    "    price.append(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5d813ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Laptop Name  Price(₹)  \\\n",
      "1   HP Envy 11th Gen Core i7 Processor 13.3-inch (...  1,23,350   \n",
      "2   Dell XPS 9370 13.3-inch FHD Display Thin & Lig...  1,14,990   \n",
      "3   HP Omen X 2S Core i9 9th Gen 15.6-inch Dual Sc...  3,45,000   \n",
      "4   Lenovo IdeaPad S540 11th Gen Intel Core i7 13....    77,990   \n",
      "5   Mi Notebook Horizon Edition 14 Intel Core i7-1...    57,990   \n",
      "6   Fujitsu UH-X 11th Gen Intel Core i7 13.3” (33....  1,07,990   \n",
      "7   MSI GF75 Thin, Intel i7-10750H, 17.3\" (43.9 cm...    74,990   \n",
      "8   ASUS TUF Dash F15 (2021) 15.6\" (39.62 cm) FHD ...  1,09,990   \n",
      "9   HP Pavilion 13(2021) 11th Gen Intel Core i7 La...    92,490   \n",
      "10  ASUS TUF Dash F15 (2021), 15.6-inch (39.62 cms...    85,990   \n",
      "\n",
      "                Rating  \n",
      "1   4.1 out of 5 stars  \n",
      "2   2.8 out of 5 stars  \n",
      "3   4.5 out of 5 stars  \n",
      "4   4.3 out of 5 stars  \n",
      "5   4.2 out of 5 stars  \n",
      "6   4.3 out of 5 stars  \n",
      "7   4.0 out of 5 stars  \n",
      "8   3.7 out of 5 stars  \n",
      "9   5.0 out of 5 stars  \n",
      "10  4.4 out of 5 stars  \n"
     ]
    }
   ],
   "source": [
    "LaptopDF = pd.DataFrame({\"Laptop Name\":title[0:10],\"Price(₹)\":price[0:10],\"Rating\":Rating[0:10]})\n",
    "LaptopDF.reset_index(drop=True,inplace = True)\n",
    "LaptopDF.index+= 1\n",
    "print(LaptopDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d691cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e8b1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
